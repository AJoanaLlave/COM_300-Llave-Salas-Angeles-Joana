{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AJoanaLlave/COM_300-Llave-Salas-Angeles-Joana/blob/main/laboratorio6_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 6 sis420 \"Aprendizaje por refuerzo\"\n",
        "**Alumnas:** Llave Salas Ángeles Joana [link github](https://)\n",
        "         Pereira Cuba Claudia [link github](https://github.com/clpereirac/SIS420-IA/tree/main/Laboratorios/Lab6AprendizajePorRefuerzo)"
      ],
      "metadata": {
        "id": "RzDAgvVE8dRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "Av9IlpyI0BIG",
        "outputId": "607137e1-ff2a-4c0e-f0d4-96dd54d29133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NB39Q8uh88VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRUEBITAAAAAA"
      ],
      "metadata": {
        "id": "GkyFaHLRTBUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def train(episodes):\n",
        "    # Se inicializa el entorno \"Surround\" de Atari con imagen en RGB\n",
        "    env = gym.make(\"ALE/Surround-v5\", obs_type=\"rgb\")\n",
        "\n",
        "    # Obtenemos las dimensiones del espacio de observación y el tamaño del espacio de acción\n",
        "    state_shape = env.observation_space.shape\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Creamos la tabla Q inicializada en cero. Las filas son los estados y las columnas son las acciones\n",
        "    q_table = np.zeros((210, 160, action_size))\n",
        "\n",
        "    # Parámetros de Q-learning\n",
        "    learning_rate = 0.3  # Tasa de aprendizaje\n",
        "    discount_factor = 0.9  # Factor de descuento (para recompensas futuras)\n",
        "    epsilon = 1.0  # Probabilidad de explorar (acciones aleatorias)\n",
        "    epsilon_decay_rate = 0.0003  # Decaimiento de epsilon, para que explore menos conforme pasa el tiempo\n",
        "    rng = np.random.default_rng()  # Generador de números aleatorios\n",
        "\n",
        "    # Array para guardar las recompensas de cada episodio\n",
        "    rewards_por_episode = np.zeros(episodes)\n",
        "\n",
        "    # Bucle de entrenamiento (todos los episodios)\n",
        "    for i in range(episodes):\n",
        "        # Reiniciamos el entorno\n",
        "        env.close()\n",
        "        if (i + 1) % 100 == 0:\n",
        "            # Cada 100 episodios, mostramos la simulación (rendering)\n",
        "            env = gym.make(\"ALE/Surround-v5\", obs_type=\"rgb\", render_mode=\"human\")\n",
        "        else:\n",
        "            env = gym.make(\"ALE/Surround-v5\", obs_type=\"rgb\")\n",
        "\n",
        "        # Obtenemos el estado inicial y lo convertimos a escala de grises (simplificación)\n",
        "        state, _ = env.reset()\n",
        "        state = state.mean(axis=2).astype(int)  # Promediamos los 3 canales de color (RGB) para simplificar\n",
        "\n",
        "        # Variables para saber si el episodio terminó o fue truncado\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Bucle de cada paso en el episodio\n",
        "        while not terminated and not truncated:\n",
        "            # Si epsilon es alto, exploramos (seleccionamos una acción aleatoria), si no, explotamos (seleccionamos la mejor acción según la tabla Q)\n",
        "            if rng.random() < epsilon:\n",
        "                action = env.action_space.sample()  # Exploración (acción aleatoria)\n",
        "            else:\n",
        "                # Explotación: seleccionamos la acción que tiene el mejor valor de Q en la tabla\n",
        "                action = np.argmax(q_table[state.mean(axis=0), state.mean(axis=1)])\n",
        "\n",
        "            # Realizamos la acción y obtenemos el nuevo estado y la recompensa\n",
        "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            new_state = new_state.mean(axis=2).astype(int)  # Convertimos a escala de grises\n",
        "\n",
        "            # Actualizamos la tabla Q usando la fórmula estándar de Q-learning\n",
        "            best_next_action = np.argmax(q_table[new_state.mean(axis=0), new_state.mean(axis=1)])\n",
        "            q_table[state.mean(axis=0), state.mean(axis=1), action] += learning_rate * (\n",
        "                reward + discount_factor * q_table[new_state.mean(axis=0), new_state.mean(axis=1), best_next_action]\n",
        "                - q_table[state.mean(axis=0), state.mean(axis=1), action]\n",
        "            )\n",
        "\n",
        "            # El nuevo estado pasa a ser el actual\n",
        "            state = new_state\n",
        "\n",
        "        # Reducimos epsilon poco a poco para hacer que el agente explore menos conforme aprende\n",
        "        epsilon = max(epsilon - epsilon_decay_rate, 0.01)\n",
        "\n",
        "        # Guardamos la recompensa obtenida en este episodio\n",
        "        rewards_por_episode[i] = reward\n",
        "\n",
        "        # Imprimimos un poco el progreso cada 50 episodios\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"Episodio: {i + 1} - Recompensa: {rewards_por_episode[i]}\")\n",
        "\n",
        "    # Cerramos el entorno después de entrenar\n",
        "    env.close()\n",
        "\n",
        "    # Mostramos la tabla Q final después de todo el entrenamiento\n",
        "    print(\"Tabla Q resultante después del entrenamiento:\")\n",
        "    print(q_table)\n",
        "\n",
        "    # Calculamos y graficamos la suma de las recompensas acumuladas por cada bloque de 50 episodios\n",
        "    suma_rewards = np.zeros(episodes)\n",
        "    for t in range(episodes):\n",
        "        suma_rewards[t] = np.sum(rewards_por_episode[max(0, t - 50):(t + 1)])\n",
        "\n",
        "    # Graficamos las recompensas acumuladas a lo largo de los episodios\n",
        "    plt.plot(suma_rewards)\n",
        "    plt.xlabel('Episodios')\n",
        "    plt.ylabel('Suma de recompensas acumuladas')\n",
        "    plt.title('Evolución de las recompensas acumuladas durante el entrenamiento')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Entrenamos el modelo con 5000 episodios\n",
        "    train(5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "GmYj7JHn9i8O",
        "outputId": "2857ff99-c959-4c7b-b4c1-1b05330022d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NamespaceNotFound",
          "evalue": "Namespace ALE not found. Have you installed the proper package for ALE?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNamespaceNotFound\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3d4ebdfd1400>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Entrenamos el modelo con 5000 episodios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-3d4ebdfd1400>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Se inicializa el entorno \"Surround\" de Atari con imagen en RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ALE/Surround-v5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Obtenemos las dimensiones del espacio de observación y el tamaño del espacio de acción\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;31m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0menv_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menv_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         raise error.Error(\n\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"No registered env with id: {env_name}. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;34m\"\"\"Check if an env exists in a namespace. If it doesn't, print a helpful error message.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;31m# First check if the namespace exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0m_check_namespace_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# Then check if the name exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_namespace_exists\u001b[0;34m(ns)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Have you installed the proper package for {ns}?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespaceNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Namespace {ns} not found. {suggestion_msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNamespaceNotFound\u001b[0m: Namespace ALE not found. Have you installed the proper package for ALE?"
          ]
        }
      ]
    }
  ]
}